{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.4-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from kagglehub) (24.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from kagglehub) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kagglehub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kagglehub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kagglehub) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Downloading kagglehub-0.3.4-py3-none-any.whl (43 kB)\n",
      "Installing collected packages: kagglehub\n",
      "Successfully installed kagglehub-0.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\P'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\P'\n",
      "C:\\Users\\rosch\\AppData\\Local\\Temp\\ipykernel_12924\\3884334018.py:7: SyntaxWarning: invalid escape sequence '\\P'\n",
      "  save_dir = \"D:\\Projects\\Machine Learning Applications for Soccer\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Overview:\n",
      "Total players: 32417\n",
      "\n",
      "Career Phase Distribution:\n",
      "Career_Phase\n",
      "twilight        16640\n",
      "peak             7123\n",
      "development      6146\n",
      "breakthrough     2461\n",
      "unknown            47\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Position Distribution:\n",
      "Position\n",
      "Defender      10328\n",
      "Midfield       9324\n",
      "Attack         8863\n",
      "Goalkeeper     3718\n",
      "Missing         184\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Average Values by Career Phase:\n",
      "Career_Phase\n",
      "breakthrough    2.091112e+06\n",
      "development     2.761342e+06\n",
      "peak            2.650386e+06\n",
      "twilight        7.320232e+05\n",
      "unknown         2.284375e+05\n",
      "Name: Current_Value, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "#D:\\Projects\\Machine Learning Applications for Soccer\n",
    "save_dir = \"D:\\Projects\\Machine Learning Applications for Soccer\"\n",
    "\n",
    "# Download the dataset\n",
    "path = kagglehub.dataset_download(\"davidcariboo/player-scores\")\n",
    "\n",
    "def create_comprehensive_dataset():\n",
    "    # Load all required datasets\n",
    "    players = pd.read_csv(f\"{path}/players.csv\")\n",
    "    appearances = pd.read_csv(f\"{path}/appearances.csv\")\n",
    "    clubs = pd.read_csv(f\"{path}/clubs.csv\")\n",
    "    \n",
    "    # Process player information\n",
    "    # Convert date_of_birth to datetime and calculate age\n",
    "    players['date_of_birth'] = pd.to_datetime(players['date_of_birth'])\n",
    "    players['age'] = (datetime.now() - players['date_of_birth']).dt.days / 365.25\n",
    "    \n",
    "    # Define career phases\n",
    "    def get_career_phase(age):\n",
    "        if pd.isna(age):\n",
    "            return 'unknown'\n",
    "        if age <= 21:\n",
    "            return 'breakthrough'\n",
    "        elif age <= 25:\n",
    "            return 'development'\n",
    "        elif age <= 29:\n",
    "            return 'peak'\n",
    "        else:\n",
    "            return 'twilight'\n",
    "    \n",
    "    players['career_phase'] = players['age'].apply(get_career_phase)\n",
    "    \n",
    "    # Calculate performance metrics from appearances\n",
    "    performance = appearances.groupby('player_id').agg({\n",
    "        'goals': 'sum',\n",
    "        'assists': 'sum',\n",
    "        'minutes_played': 'sum',\n",
    "        'yellow_cards': 'sum',\n",
    "        'red_cards': 'sum',\n",
    "        'game_id': 'count'  # number of appearances\n",
    "    }).reset_index()\n",
    "    performance.rename({'game_id': 'total_appearances'}, axis=1, inplace=True)\n",
    "    \n",
    "    # Calculate per-game metrics\n",
    "    performance['goals_per_game'] = performance['goals'] / performance['total_appearances']\n",
    "    performance['assists_per_game'] = performance['assists'] / performance['total_appearances']\n",
    "    performance['minutes_per_game'] = performance['minutes_played'] / performance['total_appearances']\n",
    "    \n",
    "    # Get the most recent season's performance\n",
    "    appearances['date'] = pd.to_datetime(appearances['date'])\n",
    "    appearances['season'] = appearances['date'].dt.year\n",
    "    recent_performance = appearances[appearances['season'] == appearances['season'].max()].groupby('player_id').agg({\n",
    "        'goals': 'sum',\n",
    "        'assists': 'sum',\n",
    "        'minutes_played': 'sum',\n",
    "        'yellow_cards': 'sum',\n",
    "        'red_cards': 'sum',\n",
    "        'game_id': 'count'\n",
    "    }).reset_index()\n",
    "    recent_performance = recent_performance.add_prefix('recent_')\n",
    "    recent_performance = recent_performance.rename(columns={'recent_player_id': 'player_id'})\n",
    "    \n",
    "    # Combine all information\n",
    "    comprehensive_df = players.merge(performance, on='player_id', how='left')\n",
    "    comprehensive_df = comprehensive_df.merge(recent_performance, on='player_id', how='left')\n",
    "    comprehensive_df = comprehensive_df.merge(\n",
    "        clubs[['club_id', 'name', 'domestic_competition_id', 'squad_size', 'average_age', \n",
    "               'total_market_value', 'stadium_name', 'coach_name']],\n",
    "        left_on='current_club_id',\n",
    "        right_on='club_id',\n",
    "        how='left'\n",
    "    ).rename(columns={'name_y': 'club_name', 'name_x': 'player_name'})\n",
    "    \n",
    "    # Select and rename final columns\n",
    "    final_columns = {\n",
    "        # Player Information\n",
    "        'player_id': 'ID',\n",
    "        'player_name': 'Name',\n",
    "        'age': 'Age',\n",
    "        'career_phase': 'Career_Phase',\n",
    "        'position': 'Position',\n",
    "        'sub_position': 'Sub_Position',\n",
    "        'foot': 'Preferred_Foot',\n",
    "        'height_in_cm': 'Height',\n",
    "        'country_of_citizenship': 'Nationality',\n",
    "        'date_of_birth': 'Birth_Date',\n",
    "        \n",
    "        # Contract Information\n",
    "        'contract_expiration_date': 'Contract_Expiry',\n",
    "        'market_value_in_eur': 'Current_Value',\n",
    "        'highest_market_value_in_eur': 'Peak_Value',\n",
    "        \n",
    "        # Club Information\n",
    "        'club_name': 'Current_Club',\n",
    "        'domestic_competition_id': 'League_ID',\n",
    "        'squad_size': 'Squad_Size',\n",
    "        'average_age': 'Team_Average_Age',\n",
    "        'total_market_value': 'Team_Total_Value',\n",
    "        'coach_name': 'Coach',\n",
    "        \n",
    "        # Career Performance\n",
    "        'total_appearances': 'Career_Games',\n",
    "        'minutes_played': 'Career_Minutes',\n",
    "        'goals': 'Career_Goals',\n",
    "        'assists': 'Career_Assists',\n",
    "        'yellow_cards': 'Career_Yellows',\n",
    "        'red_cards': 'Career_Reds',\n",
    "        'goals_per_game': 'Goals_Per_Game',\n",
    "        'assists_per_game': 'Assists_Per_Game',\n",
    "        'minutes_per_game': 'Minutes_Per_Game',\n",
    "        \n",
    "        # Recent Performance\n",
    "        'recent_goals': 'Recent_Goals',\n",
    "        'recent_assists': 'Recent_Assists',\n",
    "        'recent_minutes_played': 'Recent_Minutes',\n",
    "        'recent_yellow_cards': 'Recent_Yellows',\n",
    "        'recent_red_cards': 'Recent_Reds',\n",
    "        'recent_game_id': 'Recent_Games'\n",
    "    }\n",
    "    \n",
    "    # Select only columns that exist in the dataframe\n",
    "    existing_columns = [col for col in final_columns.keys() if col in comprehensive_df.columns]\n",
    "    comprehensive_df = comprehensive_df[existing_columns].rename(columns=final_columns)\n",
    "    \n",
    "    # Save the dataset\n",
    "    comprehensive_df.to_csv(save_dir + '/player_database.csv', index=False)\n",
    "    \n",
    "    return comprehensive_df\n",
    "\n",
    "# Create the dataset and display summary\n",
    "df = create_comprehensive_dataset()\n",
    "\n",
    "print(\"\\nDataset Overview:\")\n",
    "print(f\"Total players: {len(df)}\")\n",
    "print(\"\\nCareer Phase Distribution:\")\n",
    "print(df['Career_Phase'].value_counts())\n",
    "print(\"\\nPosition Distribution:\")\n",
    "print(df['Position'].value_counts())\n",
    "print(\"\\nAverage Values by Career Phase:\")\n",
    "print(df.groupby('Career_Phase')['Current_Value'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing players: 100%|██████████████████████████████████████████████████████| 32417/32417 [2:56:35<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Dataset Overview:\n",
      "Total players: 32417\n",
      "\n",
      "Columns added:\n",
      "- Player URLs\n",
      "- Injury history\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from time import sleep\n",
    "from random import uniform\n",
    "\n",
    "class SoccerDataScraper:\n",
    "    def __init__(self):\n",
    "        # Initialize logging\n",
    "        logging.basicConfig(\n",
    "            filename='scraper.log',\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        \n",
    "        # Basic setup\n",
    "        self.save_dir = \"D:/Projects/Machine Learning Applications for Soccer\"\n",
    "        self.base_transfermarkt_url = \"https://www.transfermarkt.com\"\n",
    "        self.base_fbref_url = \"https://fbref.com\"\n",
    "        \n",
    "        # Initialize session for better performance\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "        \n",
    "        # Load database\n",
    "        self.db_path = os.path.join(self.save_dir, 'player_database.csv')\n",
    "        self.load_database()\n",
    "\n",
    "    def load_database(self):\n",
    "        \"\"\"Load and prepare the database\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.db_path, encoding='utf-8')\n",
    "            \n",
    "            # Add new columns if they don't exist\n",
    "            new_columns = ['transfermarkt_url', 'fbref_url', 'injury_history']\n",
    "            for col in new_columns:\n",
    "                if col not in self.df.columns:\n",
    "                    self.df[col] = None\n",
    "                    \n",
    "            # Save the updated structure\n",
    "            self.df.to_csv(self.db_path, index=False, encoding='utf-8')\n",
    "            logging.info(f\"Database loaded successfully with {len(self.df)} players\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading database: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def make_request(self, url, retries=3, delay=1):\n",
    "        \"\"\"Make HTTP request with retry logic\"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = self.session.get(url)\n",
    "                response.raise_for_status()\n",
    "                sleep(uniform(0.5, delay))  # Random delay between requests\n",
    "                return response\n",
    "            except requests.RequestException as e:\n",
    "                if attempt == retries - 1:\n",
    "                    logging.error(f\"Failed to fetch {url} after {retries} attempts: {str(e)}\")\n",
    "                    raise\n",
    "                sleep(uniform(1, 3))  # Longer delay between retries\n",
    "        return None\n",
    "\n",
    "    def find_player_urls(self, player_name):\n",
    "        \"\"\"Find URLs for a player\"\"\"\n",
    "        try:\n",
    "            search_name = player_name.replace(' ', '+')\n",
    "            search_url = f\"{self.base_transfermarkt_url}/schnellsuche/ergebnis/schnellsuche?query={search_name}\"\n",
    "            \n",
    "            response = self.make_request(search_url)\n",
    "            if not response:\n",
    "                return None\n",
    "                \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            player_link = soup.find('table', class_='items')\n",
    "            \n",
    "            urls = {'transfermarkt_url': None, 'fbref_url': None}\n",
    "            \n",
    "            if player_link and (link := player_link.find('a', class_='spielprofil_tooltip')):\n",
    "                urls['transfermarkt_url'] = self.base_transfermarkt_url + link['href']\n",
    "            \n",
    "            return urls\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error finding URLs for {player_name}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def get_injury_history(self, url):\n",
    "        \"\"\"Scrape injury history from Transfermarkt\"\"\"\n",
    "        try:\n",
    "            response = self.make_request(url + '/verletzungen')\n",
    "            if not response:\n",
    "                return []\n",
    "                \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            injury_table = soup.find('table', class_='items')\n",
    "            \n",
    "            injuries = []\n",
    "            if injury_table:\n",
    "                for row in injury_table.find_all('tr')[1:]:\n",
    "                    cols = row.find_all('td')\n",
    "                    if len(cols) >= 5:\n",
    "                        injuries.append({\n",
    "                            'season': cols[0].text.strip(),\n",
    "                            'injury_type': cols[1].text.strip(),\n",
    "                            'days_missed': cols[4].text.strip()\n",
    "                        })\n",
    "            \n",
    "            return injuries\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error scraping injury data from {url}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def process_player(self, args):\n",
    "        \"\"\"Process a single player\"\"\"\n",
    "        player, index = args\n",
    "        try:\n",
    "            # Find URLs if not already present\n",
    "            if pd.isna(self.df.at[index, 'transfermarkt_url']):\n",
    "                urls = self.find_player_urls(player['Name'])\n",
    "                if urls:\n",
    "                    self.df.at[index, 'transfermarkt_url'] = urls['transfermarkt_url']\n",
    "                    self.df.at[index, 'fbref_url'] = urls['fbref_url']\n",
    "            \n",
    "            # Get injury history if URL exists\n",
    "            if pd.notna(self.df.at[index, 'transfermarkt_url']) and pd.isna(self.df.at[index, 'injury_history']):\n",
    "                injuries = self.get_injury_history(self.df.at[index, 'transfermarkt_url'])\n",
    "                self.df.at[index, 'injury_history'] = str(injuries)\n",
    "            \n",
    "            return index, True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing player {player['Name']}: {str(e)}\")\n",
    "            return index, False\n",
    "\n",
    "    def enhance_player_database(self, max_workers=5):\n",
    "        \"\"\"Process the database using parallel processing\"\"\"\n",
    "        try:\n",
    "            total_players = len(self.df)\n",
    "            processed_count = 0\n",
    "            \n",
    "            # Create tasks for parallel processing\n",
    "            tasks = [(row, idx) for idx, row in self.df.iterrows()]\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                futures = [executor.submit(self.process_player, task) for task in tasks]\n",
    "                \n",
    "                # Process results with progress bar\n",
    "                with tqdm(total=total_players, desc=\"Processing players\") as pbar:\n",
    "                    for future in as_completed(futures):\n",
    "                        index, success = future.result()\n",
    "                        processed_count += 1\n",
    "                        pbar.update(1)\n",
    "                        \n",
    "                        # Save checkpoint every 100 players\n",
    "                        if processed_count % 100 == 0:\n",
    "                            self.df.to_csv(self.db_path, index=False, encoding='utf-8')\n",
    "                            logging.info(f\"Checkpoint saved at {processed_count} players\")\n",
    "            \n",
    "            # Final save\n",
    "            self.df.to_csv(self.db_path, index=False, encoding='utf-8')\n",
    "            logging.info(\"Database enhancement completed successfully\")\n",
    "            \n",
    "            return self.df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in database enhancement: {str(e)}\")\n",
    "            self.df.to_csv(self.db_path, index=False, encoding='utf-8')\n",
    "            raise\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        scraper = SoccerDataScraper()\n",
    "        enhanced_df = scraper.enhance_player_database()\n",
    "        \n",
    "        print(\"\\nEnhanced Dataset Overview:\")\n",
    "        print(f\"Total players: {len(enhanced_df)}\")\n",
    "        print(\"\\nColumns added:\")\n",
    "        print(\"- Player URLs\")\n",
    "        print(\"- Injury history\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcess interrupted by user. Progress has been saved.\")\n",
    "        logging.warning(\"Process interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {str(e)}\")\n",
    "        print(\"Progress has been saved up to the last successful update.\")\n",
    "        logging.error(f\"Main process error: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
